python:
    1.1 python基本的数据结构和特点
    1.2 深拷贝浅拷贝出现的原因
    1.3 常用的自带库内的数据结构和应用场景
    1.4 内存管理
    1.5 多进程和多线程

pytorch：
    2.1 pytorch模型训练的流程
    2.2 pytorch的DistributedDataParallel和DataParallel的联系和区别

机器学习部分：
    3.1 LR在二分类时，生成两个概率和生成一个概率用阈值截断有什么不同
    3.2 PCA和LR,SVM有什么区别和联系
    3.3 LR在二分类情况下的loss
    3.4 多分类场景的常用loss
    3.5 GBDT和xgboost的原理是什么
    3.6 决策树和LR哪一个泛化性能更好，为什么 
    3.7 强化学习有了解过吗
    3.8 正则化，L1正则和L2正则的联系和区别
    3.9 简要说说LR, SVM, 决策树各自的应用场景
    3.10 为什么会过拟合，有什么手段去缓解过拟合
    3.11 数据归一化的作用是什么

深度学习部分：
    4.1 文本生成：
        4.1.1 Transformer在训练时decoder过程如何避免模型看到当前position后面的内容
        4.1.2 如果涉及到多语种的问题，有一些语言的corpus很小，有什么解决办法
        4.1.3 有什么可控的生成手段，比如生成商家或者商品的推荐语想要突出某些特点,前提是要保证生成内容的可靠性
        4.1.4 采用了哪些策略来实现多样化的文本生成，具体是怎么做的
        4.1.5 生成长文本的时候比较容易出现某些句子发生重复，怎么解决
        4.1.6 有什么评估生成质量的方法
        4.1.7 Decorder有什么解码策略
        4.1.8 OOV问题有什么解决的方法
    4.2 知识图谱：
        4.2.1 知识图谱的表示学习有什么方法，优缺点
        4.2.2 从0开始建一个知识图谱的流程是怎样的，你会使用哪些技术
        4.2.3 如何评估知识图谱表示学习的好坏？
        4.2.4 采用更大的训练数据和模型体量，知识图谱的表示学习你是否遇到了瓶颈
        4.2.5 知识图谱的表示学习最近自己有什么想法或者创新点么
    4.3 关系抽取
        4.3.1 关系抽取的常用方法你了解哪些
        4.3.2 Casrel和TPLinker有了解过么
        4.3.3 如果一句话抽取出来的实体很多，两两配对的开销比较大，怎么办？
        4.3.4 目前的关系抽取你有什么正在做的工作么
    4.4 梯度下降
        4.4.1 SGD和牛顿逼近法有什么区别
        4.4.2 SGD和Adam的区别
    4.5 BERT & Transformer
        4.5.1 BERT的预训练任务是什么
        4.5.2 BERT有哪些优化模型，分别从什么角度进行了优化？
        4.5.3 多头注意力的物理意义是什么，如何检验这种意义，如果想控制不同的HEAD的走向，怎么做？
        4.5.4 残差连接是在LN层之前还是之后
        4.5.5 Transformer为了缓解过拟合有过怎样的设计
        4.5.6 注意力计算的过程中根号d_k的意义是什么
        4.5.7 Self-Attention的时间复杂度
     4.6 CRF
        4.6.1 CRF的loss是什么
        4.6.2 CRF和HMM的区别
        4.6.3 简要说一下维特比解码，时间复杂度是什么

算法部分：
    现场算法题目的主要考点：动态规划，双指针，回溯和滑动窗口
    只有华为考了树结构，其余公司的现场算法题目全部是字符串或者数组相关
    一般都是leetcode中等难度，字节跳动全部是leetcode困难难度
